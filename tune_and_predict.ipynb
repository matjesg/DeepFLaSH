{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tune_and_predict.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matjesg/DeepFLaSH/blob/master/tune_and_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "EzVJo3rXc_cq"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook is optmizied to be executed on Google Colab (https://colab.research.google.com).\n",
        "\n",
        "\n",
        "*   Please read the instructions carefully.\n",
        "*   Press the the *play* butten to execute the cells. It will show up between \\[     \\] on the left side of the code cells. \n",
        "*   Run the cells consecutively.\n",
        "\n",
        "**Note:** You can predict your images without fine-tuning the model. For this, skip the Section 2  *Train model on the new data*."
      ]
    },
    {
      "metadata": {
        "colab_type": "toc",
        "id": "-KuRASf8KdW2"
      },
      "cell_type": "markdown",
      "source": [
        ">[Configuration](#scrollTo=rPRgwBd5tmGp)\n",
        "\n",
        ">>[Set up Google Colab Environment](#scrollTo=Zm5wDM15c_cw)\n",
        "\n",
        ">>[Choose base model](#scrollTo=c5ICgEznc_dB)\n",
        "\n",
        ">[Train model on the new data](#scrollTo=6e59NnbQc_dL)\n",
        "\n",
        ">>[Provide image training data](#scrollTo=V1588m6AtbhK)\n",
        "\n",
        ">>>[Upload your images and masks (segmentation maps)](#scrollTo=5pLJdaY1Ymmh)\n",
        "\n",
        ">>>[Use example images](#scrollTo=uB9vyeR3bVI0)\n",
        "\n",
        ">>>[Plot images and masks](#scrollTo=8cD0AY6uZn71)\n",
        "\n",
        ">>[Model training](#scrollTo=fV6yuPVuuL9P)\n",
        "\n",
        ">>>[Check results on train data](#scrollTo=hv5bXNnJc_dX)\n",
        "\n",
        ">>>[Plot all images and joined mask](#scrollTo=5n3FfCHnc_do)\n",
        "\n",
        ">[Create segmentation maps for new images](#scrollTo=MAVMhDs1c_dg)\n",
        "\n",
        ">>[Compare segmentation results](#scrollTo=Tg09Afx3Mwra)\n",
        "\n",
        ">>[Save and download predicted masks](#scrollTo=NZ7kRhs9c_ds)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rPRgwBd5tmGp"
      },
      "cell_type": "markdown",
      "source": [
        "# Configuration\n",
        "In this section, you will set up the training environment and choose your base model."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Zm5wDM15c_cw"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up Google Colab Environment"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UCPo3P9Mc_c0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/matjesg/DeepFLaSH.git\n",
        "import os\n",
        "import sys\n",
        "ROOT_DIR = os.path.abspath(\"DeepFLaSH\")\n",
        "sys.path.append(ROOT_DIR)\\\n",
        "    \n",
        "import numpy as np\n",
        "from unet import utils\n",
        "from unet import colab_utils\n",
        "from unet import sim_measures\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "c5ICgEznc_dB"
      },
      "cell_type": "markdown",
      "source": [
        "## Choose base model\n",
        "\n",
        "Look at the images and masks (segmentation maps) below. Which are more similar to yours?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "iRD035oENxhu"
      },
      "cell_type": "markdown",
      "source": [
        "* [cFOS_Wue](https://drive.google.com/open?id=1u1jAqxRpQh2hjE0W2vdHNCyhQsM5uAis): \n",
        "Trained on 36 image-mask pairs of cFOS labels in the dorsal hippocampus (including 12 images of each sub-region: dentate gyrus, CA3 and CA1). Masks for training were prepared by five independent experts. Images were acquired using laser-scanning confocal microscopy with a resolution of 1.6 pixel per µm.\n",
        "    \n",
        "    <img src=\"https://raw.githubusercontent.com/matjesg/DeepFLaSH/master/assets/cFOS_Wue.png\" width=\"250\" height=\"250\" alt=\"cFOS_Wue\">\n",
        "    <img src=\"https://raw.githubusercontent.com/matjesg/DeepFLaSH/master/assets/cFOS_Wue_mask.png\" width=\"250\" height=\"250\" alt=\"cFOS_Wue_mask\">\n",
        "\n",
        "* [cFOS_Inns1](https://drive.google.com/open?id=1n6oGHaIvhbcBtzrkgWT6igg8ZXSOvE0D): Fine-tuned on [cFOS_Wue](https://drive.google.com/open?id=1u1jAqxRpQh2hjE0W2vdHNCyhQsM5uAis)\n",
        "with five image-mask pairs of cFOS labels in the amygdala. Masks for fine-tuning were prepared by one expert. Images acquired using epifluorescence microscopy with a resolution of 1 pixel per µm.\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/matjesg/DeepFLaSH/master/assets/cFOS_Inns1.png\" width=\"250\" height=\"250\" alt=\"cFOS_Inns1\">\n",
        "    <img src=\"https://raw.githubusercontent.com/matjesg/DeepFLaSH/master/assets/cFOS_Inns1_mask.png\" width=\"250\" height=\"250\" alt=\"cFOS_Inns1_mask\">\n",
        "\n",
        "* [cFOS_Inns2](https://drive.google.com/open?id=1TGxZC93YUP1kp1xmboxl6fJEqU4oDRzP):\n",
        "Fine-tuned on [cFOS_Wue](https://drive.google.com/open?id=1u1jAqxRpQh2hjE0W2vdHNCyhQsM5uAis)\n",
        "with five image-mask pairs of cFOS labels in the infralimbic cortex. Masks for fine-tuning were prepared by one expert. Images acquired using epifluorescence microscopy with a resolution of 2 pixel per µm.\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/matjesg/DeepFLaSH/master/assets/cFOS_Inns2.png\" width=\"250\" height=\"250\" alt=\"cFOS_Inns2\">\n",
        "    <img src=\"https://raw.githubusercontent.com/matjesg/DeepFLaSH/master/assets/cFOS_Inns2_mask.png\" width=\"250\" height=\"250\" alt=\"cFOS_Inns2_mask\">\n",
        "\n",
        "* [cFOS_Mue](https://drive.google.com/open?id=1GFOsnLFY8nKDVcBTX7MvMTjoiYfhs91b):\n",
        "Fine-tuned on [cFOS_Wue](https://drive.google.com/open?id=1u1jAqxRpQh2hjE0W2vdHNCyhQsM5uAis)\n",
        "with five image-mask pairs of cFOS labels in the paraventricular nucleus of the thalamus. Masks for fine-tuning were prepared by one expert. Images acquired using laser-scanning confocal microscopy with a resolution of 0.8 pixel per µm.\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/matjesg/DeepFLaSH/master/assets/cFOS_Mue.png\" width=\"250\" height=\"250\" alt=\"cFOS_Mue\">\n",
        "    <img src=\"https://raw.githubusercontent.com/matjesg/DeepFLaSH/master/assets/cFOS_Mue_mask.png\" width=\"250\" height=\"250\" alt=\"cFOS_Mue_mask\">\n",
        "\n",
        "* [Parv](https://drive.google.com/open?id=1VtxyOXhuYVDAC8pkzx3SG9sZfvXqHDZI):\n",
        "Trained on 36 image-mask pairs of Parvalbumin-labels in the dorsal hippocampus (including 12 images of each sub-region: dentate gyrus, CA3 and CA1). Masks for training were prepared by five independent experts. Images were acquired using laser-scanning confocal microscopy with a resolution of 1.6 pixel per µm.\n",
        "    \n",
        "    <img src=\"https://raw.githubusercontent.com/matjesg/DeepFLaSH/master/assets/Parv.png\" width=\"250\" height=\"250\" alt=\"Parv\">\n",
        "    <img src=\"https://raw.githubusercontent.com/matjesg/DeepFLaSH/master/assets/Parv_mask.png\" width=\"250\" height=\"250\" alt=\"Parv\">"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BOqv3DBfVolt"
      },
      "cell_type": "markdown",
      "source": [
        "**Which model do you want to choose:**\n",
        "\n",
        "* pretrained: 'cFOS_Wue', 'cFOS_Inns1', 'cFOS_Inns2', 'cFOS_Mue' or 'Parv'\n",
        "* untrained model (no pretrained weights): 'new'"
      ]
    },
    {
      "metadata": {
        "id": "b0kOHudE0i-I",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run: \"auto\" }\n",
        "\n",
        "Model_name = 'cFOS_Wue' #@param [\"cFOS_Wue\", \"cFOS_Inns1\", \"cFOS_Inns2\", \"cFOS_Inns2\", \"cFOS_Mue\", \"Parv\", \"new\"]\n",
        "model = utils.load_unet(Model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6e59NnbQc_dL"
      },
      "cell_type": "markdown",
      "source": [
        "# Train model on the new data \n",
        "In this section, you can train your own model."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "V1588m6AtbhK"
      },
      "cell_type": "markdown",
      "source": [
        "## Provide image training data\n",
        "Either **upload your own images** or use the **transfer learning images from our repository**."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5pLJdaY1Ymmh"
      },
      "cell_type": "markdown",
      "source": [
        "### Upload your images and masks (segmentation maps)\n",
        "The training images and masks should reflect the diversity of your dataset.\n",
        "* For fine-tuning a model, we recommend at least five image-mask pairs.\n",
        "* For training a new model from scratch we recommend about 30 image-mask pairs.\n",
        "* Make sure that both images and masks follow the same naming conventions, e.g. '01_img.tif' and '01_mask.tif'.\n",
        "* Images will be resized to a 1024x1024 pixel resolution (greyscale, one channel).\n",
        "* Typical filetypes are allowed (e.g., tif, png)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fdBYmbj7ZGmh"
      },
      "cell_type": "markdown",
      "source": [
        "**Images:**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "t-Z6Or35c_dN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_names, img_list = colab_utils.upload_files()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mqCfvr89dkPH"
      },
      "cell_type": "markdown",
      "source": [
        "**Masks**:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LXB8z1wsdfFY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "msk_names, msk_list = colab_utils.upload_files()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "uB9vyeR3bVI0"
      },
      "cell_type": "markdown",
      "source": [
        "### Use example images"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8BopN6H6TY5T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_names, img_list = colab_utils.load_samples(path = 'transfer_learning/train', suffix = 'new')\n",
        "msk_names, msk_list = colab_utils.load_samples(path = 'transfer_learning/train', suffix = 'expert')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8cD0AY6uZn71"
      },
      "cell_type": "markdown",
      "source": [
        "### Plot images and masks \n",
        "\n",
        "Check if images and masks are correctly assigned. If not, adjust your filenames and upload the images and masks again."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xe5zVT7HdWTS",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "utils.plot_image_and_mask(img_names = img_names, img_list = img_list,\n",
        "                          msk_names = msk_names, msk_list = msk_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fV6yuPVuuL9P"
      },
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "IDAcroGzD2mr"
      },
      "cell_type": "markdown",
      "source": [
        "**Training duration (epochs)**\n",
        "\n",
        "One epoch is when an entire (augemented) dataset is passed through the neural network for training. \n",
        "* We recommend about 50 epochs for fine-tuning and at least 100 epochs for a new model. \n",
        "* Choose a higher number if your images are very dissimilar to the sample images above."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "j0GDWzBvD1gA",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#{ run: \"auto\" }\n",
        "epochs = 50 #@param {type:\"slider\", min:10, max:300, step:5}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pO8noKeXOqiD"
      },
      "cell_type": "markdown",
      "source": [
        "**Train model** using data augmentation (rotation, flipping, shifting) for images and masks"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "m8p-Lf2DdUiC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_generator = utils.create_generator(img_list, msk_list)\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=int(np.ceil(len(img_list)/4.)),\n",
        "                    epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hv5bXNnJc_dX"
      },
      "cell_type": "markdown",
      "source": [
        "### Check results on train data\n",
        "\n",
        "Predict masks with the U-net"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pPi9xg5Oc_dZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_train = model.predict(np.asarray(img_list))\n",
        "pred_train_list = [pred_train[i] for i in range(pred_train.shape[0])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xB4dtRWstTT7",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@markdown Calculate Jaccard Similary. The computation of the ROI-wise Jaccard Similarity may take up more time.\n",
        "pixelwise = False #@param {type:\"boolean\"}\n",
        "roiwise = True #@param {type:\"boolean\"}\n",
        "#@markdown \n",
        "\n",
        "if pixelwise:\n",
        "  jac_pix = [sim_measures.jaccard_pixelwise(a, b, threshold=0.5) for a,b \n",
        "             in zip(pred_train_list, msk_list)]\n",
        "\n",
        "if roiwise:\n",
        "  jac_roi = [sim_measures.jaccard_roiwise(a, b, threshold=0.5, min_roi_pixel=15)\n",
        "             for a,b in zip(pred_train_list, msk_list)]\n",
        " \n",
        "if pixelwise and not roiwise:\n",
        "  jac_str = [str('pixelwise %.2f' %pix) for pix in jac_pix]\n",
        "  \n",
        "if roiwise and not pixelwise: \n",
        "  jac_str = [str('ROI-wise %.2f' %roi) for roi in jac_roi]\n",
        "\n",
        "if roiwise and pixelwise:\n",
        "  jac_str = [str('pixelwise %.2f, ROI-wise: %.2f' %(pix,roi)) \n",
        "     for pix,roi in zip(jac_pix, jac_roi)]\n",
        "    \n",
        "_ = [print('Jaccard %s: %s' %(name,s)) for name,s in zip(img_names, jac_str)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5n3FfCHnc_do"
      },
      "cell_type": "markdown",
      "source": [
        "### Plot all images and joined mask\n",
        "\n",
        "The joined mask consists of the manual/expert segmentation mask and U-net prediction.\n",
        "\n",
        "Color code: \n",
        "- white = merge\n",
        "- magenta = U-net only\n",
        "- green = original/expert only"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CTYfQ1f6qhR5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "join_list = [utils.join_masks(pred_train_list[i], msk_list[i]) for i in range(len(msk_list))]\n",
        "utils.plot_image_and_mask(img_names = img_names, img_list = img_list,\n",
        "                          msk_names = jac_str, msk_list = join_list,\n",
        "                          msk_head = 'Jaccard Similarity')   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MAVMhDs1c_dg"
      },
      "cell_type": "markdown",
      "source": [
        "# Create segmentation maps for new images\n",
        "In this section, you can upload unlabelled images and predict the segmentation map (mask)."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Qr-kOS_HvaiP"
      },
      "cell_type": "markdown",
      "source": [
        "**Upload images**\n",
        "\n",
        "*   Images will be resized to a 1024x1024 pixel resolution (greyscale, one channel).\n",
        "*   Typical filetypes are allowed (e.g., tif, png)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "b-I8qielc_dh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_new_names, img_new_list = colab_utils.upload_files()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "W8GXiijPwGB0"
      },
      "cell_type": "markdown",
      "source": [
        "**Predict masks (segmentation maps) with the U-net**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DjolgK0xwFJR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_new = model.predict(np.asarray(img_new_list))\n",
        "pred_new_list = [pred_new[i] for i in range(pred_new.shape[0])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "914cBX-uwMNZ"
      },
      "cell_type": "markdown",
      "source": [
        "**Plot results**\n",
        "Look at the segmentation results of the U-net."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VH0dwbjNc_dp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "utils.plot_image_and_mask(img_names = img_new_names, img_list = img_new_list,\n",
        "                    msk_names = img_new_names, msk_list = pred_new_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Tg09Afx3Mwra"
      },
      "cell_type": "markdown",
      "source": [
        "## Compare segmentation results\n",
        "If you already have segmentation maps of the above images at your disposal, you can upload them here for comparison."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZptMiK5NOe1F"
      },
      "cell_type": "markdown",
      "source": [
        "**Upload new segmentation maps (masks)**\n",
        "\n",
        "* Make sure that both images and masks follow the same naming conventions, e.g. '01_img.tif' and '01_mask.tif'.\n",
        "* Images will be resized to a 1024x1024 pixel resolution (greyscale, one channel)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8hE415j6Mv2X",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "msk_new_names, msk_new_list = colab_utils.upload_files()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2TyeohSYO5ol"
      },
      "cell_type": "markdown",
      "source": [
        "**Plot comparison**\n",
        "\n",
        "Color code: \n",
        "- white = merge\n",
        "- magenta = U-net only\n",
        "- green = original/expert only"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Rvv6vVBuO5yx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate Jaccard Similarity\n",
        "jac_test = utils.jaccard_sim(pred_new_list, msk_new_list)\n",
        "\n",
        "join_new_list = [utils.join_masks(pred_new_list[i], msk_new_list[i]) for i in range(len(msk_list))]\n",
        "utils.plot_image_and_mask(img_names = img_new_names, img_list = img_new_list,\n",
        "                          msk_names = jac_test, msk_list = pred_new_list,\n",
        "                          msk_name = 'Jaccard Similarity')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NZ7kRhs9c_ds"
      },
      "cell_type": "markdown",
      "source": [
        "## Save and download predicted masks"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RxNm41uPc_dt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "utils.saveMasks(pred_new_list, img_new_names)\n",
        "!zip -r masks.zip masks\n",
        "files.download('masks.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}